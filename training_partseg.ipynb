{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\ntry:\n    import torchbearer\nexcept:\n    !pip install torchbearer\n    import torchbearer\nfrom torchbearer import Trial\nfrom torchbearer import callbacks\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torchbearer.callbacks.torch_scheduler import CosineAnnealingLR\nfrom torchbearer.callbacks import Callback\nimport numpy as np\nimport h5py\nfrom torch.utils.data import Dataset, DataLoader\nimport os\nfrom time import time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(torch.version.cuda)\nprint(torch.__version__)\nprint(torchbearer.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@callbacks.on_start_epoch\ndef lr_print(state):\n    opt = state[torchbearer.OPTIMIZER]\n    print(opt.param_groups[0]['lr'])\n        \n@callbacks.on_end_epoch\ndef save_state(state):\n    epoch = state[torchbearer.EPOCH]\n    torch.save(state[torchbearer.MODEL].state_dict(), f'/kaggle/working/trained_{epoch}.ckpt')\n    print('Model params saved, epoch '+str(epoch))\n    \n@callbacks.on_end_epoch\ndef miou_val(state):\n    model = state[torchbearer.MODEL]\n    miou = calc_miou(model, valloader)\n    print(miou)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import jaccard_score\n\ndef calc_miou(model, dataloader):\n    n_models = len(dataloader.dataset)\n    y_preds = np.empty(n_models*2048, dtype=int)\n    y_trues = np.empty(n_models*2048, dtype=int)\n    \n    for i,((data, labels), pids) in enumerate(dataloader):\n        batch_size = data.shape[0]\n        y_pred = model(data.to('cuda'),labels.to('cuda'))\n        y_pred = torch.flatten(y_pred.contiguous(), 0, 1)\n        _, y_pred = y_pred.max(1)\n        y_true = torch.flatten(pids.contiguous(), 0, 1)\n        \n        y_preds[i*32*2048 : i*32*2048 + batch_size*2048] = y_pred.cpu().numpy()\n        y_trues[i*32*2048 : i*32*2048 + batch_size*2048] = y_true.cpu().numpy()\n    \n    print(np.unique(y_preds, return_counts=True))\n    print(np.unique(y_trues, return_counts=True))\n    \n    iou = jaccard_score(y_trues, y_preds, average=None)*100\n    \n    n_parts = [4, 2, 2, 4, 4, 3, 3, 2, 4, 2, 6, 2, 3, 3, 3, 3]\n    cur = 0\n    miou = []\n    for n in n_parts:\n        miou.append(np.mean(iou[cur:cur+n]))\n        cur += n\n    return iou, miou","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ShapeNetPartDataset(Dataset):\n    \n    def __init__(self, dataset_path, partition_name=None):\n        \n        data = torch.empty(0)\n        labels = pids = torch.empty(0, dtype=torch.long)\n        n_files = 0\n        \n        for root, dirs, files in os.walk(dataset_path):\n            for filename in files:\n                if filename.endswith('.h5') and (partition_name is None or partition_name in filename):\n                    print('Loading '+filename)                    \n                    f = h5py.File(os.path.join(dataset_path, filename), 'r')            \n                    f_data = torch.as_tensor(f['data'][()])\n                    f_labels = torch.as_tensor(f['label'][()], dtype=torch.long).squeeze(-1)\n                    f_pids = torch.as_tensor(f['pid'][()], dtype=torch.long)\n                    data = torch.cat((data, f_data))\n                    labels = torch.cat((labels, f_labels))\n                    pids = torch.cat((pids, f_pids))\n                    n_files += 1\n        self.size = len(labels)\n        self.data = data.permute(0,2,1) # [models, points, dims] -> [models, dims, points]\n        self.labels = F.one_hot(labels).float()\n        self.pids = pids\n        strprt = '' if partition_name is None else partition_name + ' ' \n        print(str(self.data.shape[0]) + ' ' + strprt + 'models loaded from ' + str(n_files) + ' files.')\n          \n    def __getitem__(self, index):\n        if index >= len(self):\n            raise IndexError(f'self.__class__.__name__ index out of range.')\n\n        return (self.data[index], self.labels[index]), self.pids[index]\n\n    def __len__(self):\n        return self.size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_path = '/kaggle/input/shapenet-part-seg-hdf5-data/shapenet_part_seg_hdf5_data/shapenet_part_seg_hdf5_data'\n\ntrainset = ShapeNetPartDataset(dataset_path, 'train')\n#valset = ShapeNetPartDataset(dataset_path, 'val')\ntestset = ShapeNetPartDataset(dataset_path, 'test')\ntrainloader = DataLoader(trainset, batch_size=32, shuffle=True, drop_last=True)\n#valloader = DataLoader(valset, batch_size=32, shuffle=True)\ntestloader = DataLoader(testset, batch_size=32, shuffle=True)\n\nfor (data, labels), pids in testloader:\n    print(data.shape)\n    print(labels.shape)\n    print(pids.shape)\n    break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def knn(x, k):\n    inner = -2*torch.matmul(x.transpose(2, 1), x)\n    xx = torch.sum(x**2, dim=1, keepdim=True)\n    pairwise_distance = -xx - inner - xx.transpose(2, 1)\n \n    idx = pairwise_distance.topk(k=k, dim=-1)[1]   # (batch_size, num_points, k)\n    return idx\n\ndef get_graph_feature(x, k=20, idx=None, dim9=False):\n    batch_size = x.size(0)\n    num_points = x.size(2)\n    \n    x = x.view(batch_size, -1, num_points)\n    \n    if idx is None:\n        if dim9 == False:\n            idx = knn(x, k=k)   # (batch_size, num_points, k)\n        else:\n            idx = knn(x[:, 6:], k=k)\n    device = torch.device('cuda')\n    idx_base = torch.arange(0, batch_size, device=device).view(-1, 1, 1)*num_points    \n\n    idx = idx + idx_base\n\n    idx = idx.view(-1)\n \n    _, num_dims, _ = x.size()\n    \n    x = x.transpose(2, 1).contiguous()   # (batch_size, num_points, num_dims)  -> (batch_size*num_points, num_dims) #   batch_size * num_points * k + range(0, batch_size*num_points)\n    feature = x.view(batch_size*num_points, -1)[idx, :]\n    feature = feature.view(batch_size, num_points, k, num_dims) \n    x = x.view(batch_size, num_points, 1, num_dims).repeat(1, 1, k, 1)\n    feature = torch.cat((feature-x, x), dim=3).permute(0, 3, 1, 2).contiguous()\n    return feature      # (batch_size, 2*num_dims, num_points, k)\n\nclass Transform_Net(nn.Module):\n    def __init__(self):\n        super(Transform_Net, self).__init__()\n        self.k = 3\n\n        self.bn1 = nn.BatchNorm2d(64)\n        self.bn2 = nn.BatchNorm2d(128)\n        self.bn3 = nn.BatchNorm1d(1024)\n\n        self.conv1 = nn.Sequential(nn.Conv2d(6, 64, kernel_size=1, bias=False),\n                                   self.bn1,\n                                   nn.LeakyReLU(negative_slope=0.2))\n        self.conv2 = nn.Sequential(nn.Conv2d(64, 128, kernel_size=1, bias=False),\n                                   self.bn2,\n                                   nn.LeakyReLU(negative_slope=0.2))\n        self.conv3 = nn.Sequential(nn.Conv1d(128, 1024, kernel_size=1, bias=False),\n                                   self.bn3,\n                                   nn.LeakyReLU(negative_slope=0.2))\n\n        self.linear1 = nn.Linear(1024, 512, bias=False)\n        self.bn3 = nn.BatchNorm1d(512)\n        self.linear2 = nn.Linear(512, 256, bias=False)\n        self.bn4 = nn.BatchNorm1d(256)\n\n        self.transform = nn.Linear(256, 3*3)\n        torch.nn.init.constant_(self.transform.weight, 0)\n        torch.nn.init.eye_(self.transform.bias.view(3, 3))\n\n    def forward(self, x):\n        batch_size = x.size(0)\n        x = self.conv1(x)                       # (batch_size, 3*2, num_points, k) -> (batch_size, 64, num_points, k)\n        x = self.conv2(x)                       # (batch_size, 64, num_points, k) -> (batch_size, 128, num_points, k)\n        x = x.max(dim=-1, keepdim=False)[0]     # (batch_size, 128, num_points, k) -> (batch_size, 128, num_points)\n\n        x = self.conv3(x)                       # (batch_size, 128, num_points) -> (batch_size, 1024, num_points)\n        x = x.max(dim=-1, keepdim=False)[0]     # (batch_size, 1024, num_points) -> (batch_size, 1024)\n\n        x = F.leaky_relu(self.bn3(self.linear1(x)), negative_slope=0.2)     # (batch_size, 1024) -> (batch_size, 512)\n        x = F.leaky_relu(self.bn4(self.linear2(x)), negative_slope=0.2)     # (batch_size, 512) -> (batch_size, 256)\n\n        x = self.transform(x)                   # (batch_size, 256) -> (batch_size, 3*3)\n        x = x.view(batch_size, 3, 3)            # (batch_size, 3*3) -> (batch_size, 3, 3)\n\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DGCNN_partseg(nn.Module):\n    def __init__(self, k=20, emb_dims=1024, dropout=0.5, seg_num_all = 27):\n        super(DGCNN_partseg, self).__init__()\n        self.k = k\n        self.transform_net = Transform_Net()\n        \n        self.bn1 = nn.BatchNorm2d(64)\n        self.bn2 = nn.BatchNorm2d(64)\n        self.bn3 = nn.BatchNorm2d(64)\n        self.bn4 = nn.BatchNorm2d(64)\n        self.bn5 = nn.BatchNorm2d(64)\n        self.bn6 = nn.BatchNorm1d(emb_dims)\n        self.bn7 = nn.BatchNorm1d(64)\n        self.bn8 = nn.BatchNorm1d(256)\n        self.bn9 = nn.BatchNorm1d(256)\n        self.bn10 = nn.BatchNorm1d(128)\n\n        self.conv1 = nn.Sequential(nn.Conv2d(6, 64, kernel_size=1, bias=False),\n                                   self.bn1,\n                                   nn.LeakyReLU(negative_slope=0.2))\n        self.conv2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1, bias=False),\n                                   self.bn2,\n                                   nn.LeakyReLU(negative_slope=0.2))\n        self.conv3 = nn.Sequential(nn.Conv2d(64*2, 64, kernel_size=1, bias=False),\n                                   self.bn3,\n                                   nn.LeakyReLU(negative_slope=0.2))\n        self.conv4 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1, bias=False),\n                                   self.bn4,\n                                   nn.LeakyReLU(negative_slope=0.2))\n        self.conv5 = nn.Sequential(nn.Conv2d(64*2, 64, kernel_size=1, bias=False),\n                                   self.bn5,\n                                   nn.LeakyReLU(negative_slope=0.2))\n        self.conv6 = nn.Sequential(nn.Conv1d(192, emb_dims, kernel_size=1, bias=False),\n                                   self.bn6,\n                                   nn.LeakyReLU(negative_slope=0.2))\n        self.conv7 = nn.Sequential(nn.Conv1d(16, 64, kernel_size=1, bias=False),\n                                   self.bn7,\n                                   nn.LeakyReLU(negative_slope=0.2))\n        self.conv8 = nn.Sequential(nn.Conv1d(1280, 256, kernel_size=1, bias=False),\n                                   self.bn8,\n                                   nn.LeakyReLU(negative_slope=0.2))\n        self.dp1 = nn.Dropout(p=dropout)\n        self.conv9 = nn.Sequential(nn.Conv1d(256, 256, kernel_size=1, bias=False),\n                                   self.bn9,\n                                   nn.LeakyReLU(negative_slope=0.2))\n        self.dp2 = nn.Dropout(p=dropout)\n        self.conv10 = nn.Sequential(nn.Conv1d(256, 128, kernel_size=1, bias=False),\n                                   self.bn10,\n                                   nn.LeakyReLU(negative_slope=0.2))\n        self.conv11 = nn.Conv1d(128, seg_num_all, kernel_size=1, bias=False)\n        \n\n    def forward(self, x, l):\n        batch_size = x.size(0)\n        num_points = x.size(2)\n        x0 = get_graph_feature(x, k=self.k)     # (batch_size, 3, num_points) -> (batch_size, 3*2, num_points, k)\n        t = self.transform_net(x0)              # (batch_size, 3, 3)\n        x = x.transpose(2, 1)                   # (batch_size, 3, num_points) -> (batch_size, num_points, 3)\n        x = torch.bmm(x, t)                     # (batch_size, num_points, 3) * (batch_size, 3, 3) -> (batch_size, num_points, 3)\n        x = x.transpose(2, 1)                   # (batch_size, num_points, 3) -> (batch_size, 3, num_points)\n        \n        x = get_graph_feature(x, k=self.k)      # (batch_size, 3, num_points) -> (batch_size, 3*2, num_points, k)\n        x = self.conv1(x)                       # (batch_size, 3*2, num_points, k) -> (batch_size, 64, num_points, k)\n        x = self.conv2(x)                       # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points, k)\n        x1 = x.max(dim=-1, keepdim=False)[0]    # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points)\n\n        x = get_graph_feature(x1, k=self.k)     # (batch_size, 64, num_points) -> (batch_size, 64*2, num_points, k)\n        x = self.conv3(x)                       # (batch_size, 64*2, num_points, k) -> (batch_size, 64, num_points, k)\n        x = self.conv4(x)                       # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points, k)\n        x2 = x.max(dim=-1, keepdim=False)[0]    # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points)\n        \n        x = get_graph_feature(x2, k=self.k)     # (batch_size, 64, num_points) -> (batch_size, 64*2, num_points, k)\n        x = self.conv5(x)                       # (batch_size, 64*2, num_points, k) -> (batch_size, 64, num_points, k)\n        x3 = x.max(dim=-1, keepdim=False)[0]    # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points)\n\n        x = torch.cat((x1, x2, x3), dim=1)      # (batch_size, 64*3, num_points)\n        \n        x = self.conv6(x)                       # (batch_size, 64*3, num_points) -> (batch_size, emb_dims, num_points)\n        x = x.max(dim=-1, keepdim=True)[0]      # (batch_size, emb_dims, num_points) -> (batch_size, emb_dims, 1)\n        \n        l = l.view(batch_size, -1, 1)           # (batch_size, num_categoties, 1)\n        l = self.conv7(l)                       # (batch_size, num_categoties, 1) -> (batch_size, 64, 1)\n        \n        x = torch.cat((x, l), dim=1)            # (batch_size, 1088, 1)\n        x = x.repeat(1, 1, num_points)          # (batch_size, 1088, num_points)\n        \n        x = torch.cat((x, x1, x2, x3), dim=1)   # (batch_size, 1088+64*3, num_points)\n        \n        x = self.conv8(x)                       # (batch_size, 1088+64*3, num_points) -> (batch_size, 256, num_points)\n        x = self.dp1(x)\n        x = self.conv9(x)                       # (batch_size, 256, num_points) -> (batch_size, 256, num_points)\n        x = self.dp2(x)\n        x = self.conv10(x)                      # (batch_size, 256, num_points) -> (batch_size, 128, num_points)\n        x = self.conv11(x)                      # (batch_size, 256, num_points) -> (batch_size, seg_num_all, num_points)\n        \n        return x.permute(0,2,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def loss_fn(ypred, ytrue, smoothing=True):\n    ''' Calculate cross entropy loss, apply label smoothing if needed. '''\n    \n    ypred = ypred.contiguous().view(-1, ypred.shape[-1])\n    ytrue = ytrue.contiguous().flatten()\n\n    if smoothing:\n        eps = 0.2\n        n_class = ypred.size(1)\n\n        one_hot = torch.zeros_like(ypred).scatter(1, ytrue.view(-1, 1), 1)\n        one_hot = one_hot * (1 - eps) + (1 - one_hot) * eps / (n_class - 1)\n        log_prb = F.log_softmax(ypred, dim=1)\n        loss = -(one_hot * log_prb).sum(dim=1).mean()\n    else:\n        loss = F.cross_entropy(ypred, ytrue, reduction='mean')\n\n    return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model=DGCNN_partseg()\nmodel.load_state_dict(torch.load('/kaggle/input/shapenet-part-seg-hdf5-data/pretrained_partseg_199.weights'))\nmodel.conv11 = nn.Conv1d(128, 50, kernel_size=1, bias=False)\nopt = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n\n#ckpt = torch.load('/kaggle/input/shapenet-part-seg-hdf5-data/trained_partseg_ca_99.ckpt')\n#model.load_state_dict(ckpt['model'])\n#opt.load_state_dict(ckpt['opt'])\n\nscheduler = CosineAnnealingLR(200, eta_min=1e-3)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ntrial = Trial(model, opt, loss_fn, callbacks=[scheduler, lr_print], metrics=['loss']).to(device)\n\ntrial.with_generators(trainloader)\ntrial.run(epochs=200)\n\ntorch.save({'model': model.state_dict(),\n            'opt': opt.state_dict()},\n           '/kaggle/working/trained_partseg_ca_199.ckpt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.eval()\nwith torch.no_grad():\n    iou, miou = calc_miou(model, testloader)\n    print(iou)\n    print(miou)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}