{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rIXw_VYtk3b5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "try:\n",
    "    import torchbearer\n",
    "except:\n",
    "    !pip install torchbearer\n",
    "    import torchbearer\n",
    "\n",
    "from torchbearer import Trial\n",
    "from torchbearer import callbacks\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchbearer.callbacks.torch_scheduler import LambdaLR\n",
    "from torchbearer.callbacks import Callback\n",
    "import numpy as np\n",
    "import h5py\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from time import time\n",
    "import sys\n",
    "import glob\n",
    "import h5py\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CifXnL9gk9rc"
   },
   "outputs": [],
   "source": [
    "BASE_DIR = os.path.dirname(os.path.abspath(' '))\n",
    "ROOT_DIR = os.path.dirname(BASE_DIR)\n",
    "sys.path.append(BASE_DIR)\n",
    "\n",
    "DATA_PATH = os.path.join(ROOT_DIR, 'data', 'Stanford3dDataset_v1.2_Aligned_Version')\n",
    "g_classes = [x.rstrip() for x in open(os.path.join(ROOT_DIR, 'data/class_names.txt'))]\n",
    "g_class2label = {cls: i for i,cls in enumerate(g_classes)}\n",
    "g_class2color = {'ceiling':\t[0,255,0],\n",
    "                 'floor':\t[0,0,255],\n",
    "                 'wall':\t[0,255,255],\n",
    "                 'beam':        [255,255,0],\n",
    "                 'column':      [255,0,255],\n",
    "                 'window':      [100,100,255],\n",
    "                 'door':        [200,200,100],\n",
    "                 'table':       [170,120,200],\n",
    "                 'chair':       [255,0,0],\n",
    "                 'sofa':        [200,100,100],\n",
    "                 'bookcase':    [10,200,100],\n",
    "                 'board':       [200,200,200],\n",
    "                 'clutter':     [50,50,50]} \n",
    "g_easy_view_labels = [7,8,9,10,11,1]\n",
    "g_label2color = {g_classes.index(cls): g_class2color[cls] for cls in g_classes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BwmUQa3ek99Q"
   },
   "outputs": [],
   "source": [
    "def collect_point_label(anno_path, out_filename, file_format='txt'):\n",
    "    \"\"\" Convert original dataset files to data_label file (each line is XYZRGBL).\n",
    "        We aggregated all the points from each instance in the room.\n",
    "    Args:\n",
    "        anno_path: path to annotations. e.g. Area_1/office_2/Annotations/\n",
    "        out_filename: path to save collected points and labels (each line is XYZRGBL)\n",
    "        file_format: txt or numpy, determines what file format to save.\n",
    "    Returns:\n",
    "        None\n",
    "    Note:\n",
    "        the points are shifted before save, the most negative point is now at origin.\n",
    "    \"\"\"\n",
    "    points_list = []\n",
    "    \n",
    "    for f in glob.glob(os.path.join(anno_path, '*.txt')):\n",
    "        cls = os.path.basename(f).split('_')[0]\n",
    "        if cls not in g_classes: # note: in some room there is 'staris' class..\n",
    "            cls = 'clutter'\n",
    "        points = np.loadtxt(f)\n",
    "        labels = np.ones((points.shape[0],1)) * g_class2label[cls]\n",
    "        points_list.append(np.concatenate([points, labels], 1)) # Nx7\n",
    "    \n",
    "    data_label = np.concatenate(points_list, 0)\n",
    "    xyz_min = np.amin(data_label, axis=0)[0:3]\n",
    "    data_label[:, 0:3] -= xyz_min\n",
    "    \n",
    "    if file_format=='txt':\n",
    "        fout = open(out_filename, 'w')\n",
    "        for i in range(data_label.shape[0]):\n",
    "            fout.write('%f %f %f %d %d %d %d\\n' % \\\n",
    "                          (data_label[i,0], data_label[i,1], data_label[i,2],\n",
    "                           data_label[i,3], data_label[i,4], data_label[i,5],\n",
    "                           data_label[i,6]))\n",
    "        fout.close()\n",
    "    elif file_format=='numpy':\n",
    "        np.save(out_filename, data_label)\n",
    "    else:\n",
    "        print('ERROR!! Unknown file format: %s, please use txt or numpy.' % \\\n",
    "            (file_format))\n",
    "        exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yGfpzXqIk-DX"
   },
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join(ROOT_DIR, 'data/Stanford3dDataset_v1.2_Aligned_Version')\n",
    "\n",
    "def collect_indoor3d_data():\n",
    "    anno_paths = [line.rstrip() for line in open(os.path.join(ROOT_DIR, 'data/anno_paths.txt'))]\n",
    "    anno_paths = [os.path.join(DATA_PATH, p) for p in anno_paths]\n",
    "\n",
    "    output_folder = os.path.join(ROOT_DIR, 'data/stanford_indoor3d') \n",
    "    if not os.path.exists(output_folder):\n",
    "        os.mkdir(output_folder)\n",
    "\n",
    "    revise_file = os.path.join(DATA_PATH, \"Area_5/hallway_6/Annotations/ceiling_1.txt\")\n",
    "    with open(revise_file, \"r\") as f:\n",
    "        data = f.read()\n",
    "        data = data[:5545347] + ' ' + data[5545348:]\n",
    "        f.close()\n",
    "    with open(revise_file, \"w\") as f:\n",
    "        f.write(data)\n",
    "        f.close()\n",
    "\n",
    "    for anno_path in anno_paths:\n",
    "        try:\n",
    "            elements = anno_path.split('\\\\')\n",
    "            out_filename = elements[-3]+'_'+elements[-2]+'.npy'\n",
    "            collect_point_label(anno_path, os.path.join(output_folder, out_filename), 'numpy')\n",
    "            print(\"SUCCESS\")\n",
    "        except:\n",
    "            print(anno_path, 'ERROR!!')\n",
    "\n",
    "\n",
    "    print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I2Z3F_rvk-P3"
   },
   "outputs": [],
   "source": [
    "def point_label_to_obj(input_filename, out_filename, label_color=True, easy_view=False, no_wall=False):\n",
    "    \"\"\" For visualization of a room from data_label file,\n",
    "\tinput_filename: each line is X Y Z R G B L\n",
    "\tout_filename: OBJ filename,\n",
    "            visualize input file by coloring point with label color\n",
    "        easy_view: only visualize furnitures and floor\n",
    "    \"\"\"\n",
    "    data_label = np.loadtxt(input_filename)\n",
    "    data = data_label[:, 0:6]\n",
    "    label = data_label[:, -1].astype(int)\n",
    "    fout = open(out_filename, 'w')\n",
    "    for i in range(data.shape[0]):\n",
    "        color = g_label2color[label[i]]\n",
    "        if easy_view and (label[i] not in g_easy_view_labels):\n",
    "            continue\n",
    "        if no_wall and ((label[i] == 2) or (label[i]==0)):\n",
    "            continue\n",
    "        if label_color:\n",
    "            fout.write('v %f %f %f %d %d %d\\n' % \\\n",
    "                (data[i,0], data[i,1], data[i,2], color[0], color[1], color[2]))\n",
    "        else:\n",
    "            fout.write('v %f %f %f %d %d %d\\n' % \\\n",
    "                (data[i,0], data[i,1], data[i,2], data[i,3], data[i,4], data[i,5]))\n",
    "    fout.close()\n",
    "    \n",
    "# -----------------------------------------------------------------------------\n",
    "# PREPARE BLOCK DATA FOR DEEPNETS TRAINING/TESTING\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def sample_data(data, num_sample):\n",
    "    \"\"\" data is in N x ...\n",
    "        we want to keep num_samplexC of them.\n",
    "        if N > num_sample, we will randomly keep num_sample of them.\n",
    "        if N < num_sample, we will randomly duplicate samples.\n",
    "    \"\"\"\n",
    "    N = data.shape[0]\n",
    "    if (N == num_sample):\n",
    "        return data, range(N)\n",
    "    elif (N > num_sample):\n",
    "        sample = np.random.choice(N, num_sample)\n",
    "        return data[sample, ...], sample\n",
    "    else:\n",
    "        sample = np.random.choice(N, num_sample-N)\n",
    "        dup_data = data[sample, ...]\n",
    "        return np.concatenate([data, dup_data], 0), list(range(N))+list(sample)\n",
    "\n",
    "def sample_data_label(data, label, num_sample):\n",
    "    new_data, sample_indices = sample_data(data, num_sample)\n",
    "    new_label = label[sample_indices]\n",
    "    return new_data, new_label\n",
    "    \n",
    "def room2blocks(data, label, num_point, block_size=1.0, stride=1.0,\n",
    "                random_sample=False, sample_num=None, sample_aug=1):\n",
    "    \"\"\" Prepare block training data.\n",
    "    Args:\n",
    "        data: N x 6 numpy array, 012 are XYZ in meters, 345 are RGB in [0,1]\n",
    "            assumes the data is shifted (min point is origin) and aligned\n",
    "            (aligned with XYZ axis)\n",
    "        label: N size uint8 numpy array from 0-12\n",
    "        num_point: int, how many points to sample in each block\n",
    "        block_size: float, physical size of the block in meters\n",
    "        stride: float, stride for block sweeping\n",
    "        random_sample: bool, if True, we will randomly sample blocks in the room\n",
    "        sample_num: int, if random sample, how many blocks to sample\n",
    "            [default: room area]\n",
    "        sample_aug: if random sample, how much aug\n",
    "    Returns:\n",
    "        block_datas: K x num_point x 6 np array of XYZRGB, RGB is in [0,1]\n",
    "        block_labels: K x num_point x 1 np array of uint8 labels\n",
    "        \n",
    "    TODO: for this version, blocking is in fixed, non-overlapping pattern.\n",
    "    \"\"\"\n",
    "    assert(stride<=block_size)\n",
    "\n",
    "    limit = np.amax(data, 0)[0:3]\n",
    "     \n",
    "    # Get the corner location for our sampling blocks    \n",
    "    xbeg_list = []\n",
    "    ybeg_list = []\n",
    "    if not random_sample:\n",
    "        num_block_x = int(np.ceil((limit[0] - block_size) / stride)) + 1\n",
    "        num_block_y = int(np.ceil((limit[1] - block_size) / stride)) + 1\n",
    "        for i in range(num_block_x):\n",
    "            for j in range(num_block_y):\n",
    "                xbeg_list.append(i*stride)\n",
    "                ybeg_list.append(j*stride)\n",
    "    else:\n",
    "        num_block_x = int(np.ceil(limit[0] / block_size))\n",
    "        num_block_y = int(np.ceil(limit[1] / block_size))\n",
    "        if sample_num is None:\n",
    "            sample_num = num_block_x * num_block_y * sample_aug\n",
    "        for _ in range(sample_num):\n",
    "            xbeg = np.random.uniform(-block_size, limit[0]) \n",
    "            ybeg = np.random.uniform(-block_size, limit[1]) \n",
    "            xbeg_list.append(xbeg)\n",
    "            ybeg_list.append(ybeg)\n",
    "\n",
    "    # Collect blocks\n",
    "    block_data_list = []\n",
    "    block_label_list = []\n",
    "    idx = 0\n",
    "    for idx in range(len(xbeg_list)): \n",
    "       xbeg = xbeg_list[idx]\n",
    "       ybeg = ybeg_list[idx]\n",
    "       xcond = (data[:,0]<=xbeg+block_size) & (data[:,0]>=xbeg)\n",
    "       ycond = (data[:,1]<=ybeg+block_size) & (data[:,1]>=ybeg)\n",
    "       cond = xcond & ycond\n",
    "       if np.sum(cond) < 100: # discard block if there are less than 100 pts.\n",
    "           continue\n",
    "       \n",
    "       block_data = data[cond, :]\n",
    "       block_label = label[cond]\n",
    "       \n",
    "       # randomly subsample data\n",
    "       block_data_sampled, block_label_sampled = \\\n",
    "           sample_data_label(block_data, block_label, num_point)\n",
    "       block_data_list.append(np.expand_dims(block_data_sampled, 0))\n",
    "       block_label_list.append(np.expand_dims(block_label_sampled, 0))\n",
    "            \n",
    "    return np.concatenate(block_data_list, 0), \\\n",
    "           np.concatenate(block_label_list, 0)\n",
    "\n",
    "\n",
    "def room2blocks_plus(data_label, num_point, block_size, stride,\n",
    "                     random_sample, sample_num, sample_aug):\n",
    "    \"\"\" room2block with input filename and RGB preprocessing.\n",
    "    \"\"\"\n",
    "    data = data_label[:,0:6]\n",
    "    data[:,3:6] /= 255.0\n",
    "    label = data_label[:,-1].astype(np.uint8)\n",
    "    \n",
    "    return room2blocks(data, label, num_point, block_size, stride,\n",
    "                       random_sample, sample_num, sample_aug)\n",
    "   \n",
    "def room2blocks_wrapper(data_label_filename, num_point, block_size=1.0, stride=1.0,\n",
    "                        random_sample=False, sample_num=None, sample_aug=1):\n",
    "    if data_label_filename[-3:] == 'txt':\n",
    "        data_label = np.loadtxt(data_label_filename)\n",
    "    elif data_label_filename[-3:] == 'npy':\n",
    "        data_label = np.load(data_label_filename)\n",
    "    else:\n",
    "        print('Unknown file type! exiting.')\n",
    "        exit()\n",
    "    return room2blocks_plus(data_label, num_point, block_size, stride,\n",
    "                            random_sample, sample_num, sample_aug)\n",
    "\n",
    "def room2blocks_plus_normalized(data_label, num_point, block_size, stride,\n",
    "                                random_sample, sample_num, sample_aug):\n",
    "    \"\"\" room2block, with input filename and RGB preprocessing.\n",
    "        for each block centralize XYZ, add normalized XYZ as 678 channels\n",
    "    \"\"\"\n",
    "    data = data_label[:,0:6]\n",
    "    data[:,3:6] /= 255.0\n",
    "    label = data_label[:,-1].astype(np.uint8)\n",
    "    max_room_x = max(data[:,0])\n",
    "    max_room_y = max(data[:,1])\n",
    "    max_room_z = max(data[:,2])\n",
    "    \n",
    "    data_batch, label_batch = room2blocks(data, label, num_point, block_size, stride,\n",
    "                                          random_sample, sample_num, sample_aug)\n",
    "    new_data_batch = np.zeros((data_batch.shape[0], num_point, 9))\n",
    "    for b in range(data_batch.shape[0]):\n",
    "        new_data_batch[b, :, 6] = data_batch[b, :, 0]/max_room_x\n",
    "        new_data_batch[b, :, 7] = data_batch[b, :, 1]/max_room_y\n",
    "        new_data_batch[b, :, 8] = data_batch[b, :, 2]/max_room_z\n",
    "        minx = min(data_batch[b, :, 0])\n",
    "        miny = min(data_batch[b, :, 1])\n",
    "        data_batch[b, :, 0] -= (minx+block_size/2)\n",
    "        data_batch[b, :, 1] -= (miny+block_size/2)\n",
    "    new_data_batch[:, :, 0:6] = data_batch\n",
    "    return new_data_batch, label_batch\n",
    "\n",
    "\n",
    "def room2blocks_wrapper_normalized(data_label_filename, num_point, block_size=1.0, stride=1.0,\n",
    "                                   random_sample=False, sample_num=None, sample_aug=1):\n",
    "    if data_label_filename[-3:] == 'txt':\n",
    "        data_label = np.loadtxt(data_label_filename)\n",
    "    elif data_label_filename[-3:] == 'npy':\n",
    "        data_label = np.load(data_label_filename)\n",
    "    else:\n",
    "        print('Unknown file type! exiting.')\n",
    "        exit()\n",
    "    return room2blocks_plus_normalized(data_label, num_point, block_size, stride,\n",
    "                                       random_sample, sample_num, sample_aug)\n",
    "\n",
    "def room2samples(data, label, sample_num_point):\n",
    "    \"\"\" Prepare whole room samples.\n",
    "    Args:\n",
    "        data: N x 6 numpy array, 012 are XYZ in meters, 345 are RGB in [0,1]\n",
    "            assumes the data is shifted (min point is origin) and\n",
    "            aligned (aligned with XYZ axis)\n",
    "        label: N size uint8 numpy array from 0-12\n",
    "        sample_num_point: int, how many points to sample in each sample\n",
    "    Returns:\n",
    "        sample_datas: K x sample_num_point x 9\n",
    "                     numpy array of XYZRGBX'Y'Z', RGB is in [0,1]\n",
    "        sample_labels: K x sample_num_point x 1 np array of uint8 labels\n",
    "    \"\"\"\n",
    "    N = data.shape[0]\n",
    "    order = np.arange(N)\n",
    "    np.random.shuffle(order) \n",
    "    data = data[order, :]\n",
    "    label = label[order]\n",
    "\n",
    "    batch_num = int(np.ceil(N / float(sample_num_point)))\n",
    "    sample_datas = np.zeros((batch_num, sample_num_point, 6))\n",
    "    sample_labels = np.zeros((batch_num, sample_num_point, 1))\n",
    "\n",
    "    for i in range(batch_num):\n",
    "        beg_idx = i*sample_num_point\n",
    "        end_idx = min((i+1)*sample_num_point, N)\n",
    "        num = end_idx - beg_idx\n",
    "        sample_datas[i,0:num,:] = data[beg_idx:end_idx, :]\n",
    "        sample_labels[i,0:num,0] = label[beg_idx:end_idx]\n",
    "        if num < sample_num_point:\n",
    "            makeup_indices = np.random.choice(N, sample_num_point - num)\n",
    "            sample_datas[i,num:,:] = data[makeup_indices, :]\n",
    "            sample_labels[i,num:,0] = label[makeup_indices]\n",
    "    return sample_datas, sample_labels\n",
    "\n",
    "def room2samples_plus_normalized(data_label, num_point):\n",
    "    \"\"\" room2sample, with input filename and RGB preprocessing.\n",
    "        for each block centralize XYZ, add normalized XYZ as 678 channels\n",
    "    \"\"\"\n",
    "    data = data_label[:,0:6]\n",
    "    data[:,3:6] /= 255.0\n",
    "    label = data_label[:,-1].astype(np.uint8)\n",
    "    max_room_x = max(data[:,0])\n",
    "    max_room_y = max(data[:,1])\n",
    "    max_room_z = max(data[:,2])\n",
    "    #print(max_room_x, max_room_y, max_room_z)\n",
    "    \n",
    "    data_batch, label_batch = room2samples(data, label, num_point)\n",
    "    new_data_batch = np.zeros((data_batch.shape[0], num_point, 9))\n",
    "    for b in range(data_batch.shape[0]):\n",
    "        new_data_batch[b, :, 6] = data_batch[b, :, 0]/max_room_x\n",
    "        new_data_batch[b, :, 7] = data_batch[b, :, 1]/max_room_y\n",
    "        new_data_batch[b, :, 8] = data_batch[b, :, 2]/max_room_z\n",
    "        #minx = min(data_batch[b, :, 0])\n",
    "        #miny = min(data_batch[b, :, 1])\n",
    "        #data_batch[b, :, 0] -= (minx+block_size/2)\n",
    "        #data_batch[b, :, 1] -= (miny+block_size/2)\n",
    "    new_data_batch[:, :, 0:6] = data_batch\n",
    "    return new_data_batch, label_batch\n",
    "\n",
    "\n",
    "def room2samples_wrapper_normalized(data_label_filename, num_point):\n",
    "    if data_label_filename[-3:] == 'txt':\n",
    "        data_label = np.loadtxt(data_label_filename)\n",
    "    elif data_label_filename[-3:] == 'npy':\n",
    "        data_label = np.load(data_label_filename)\n",
    "    else:\n",
    "        print('Unknown file type! exiting.')\n",
    "        exit()\n",
    "    return room2samples_plus_normalized(data_label, num_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_HCCI-vdk-b5"
   },
   "outputs": [],
   "source": [
    "def save_h5(h5_filename, data, label, data_dtype='uint8', label_dtype='uint8'):\n",
    "    h5_fout = h5py.File(h5_filename)\n",
    "    h5_fout.create_dataset(\n",
    "            'data', data=data,\n",
    "            compression='gzip', compression_opts=4,\n",
    "            dtype=data_dtype)\n",
    "    h5_fout.create_dataset(\n",
    "            'label', data=label,\n",
    "            compression='gzip', compression_opts=1,\n",
    "            dtype=label_dtype)\n",
    "    h5_fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ENaBFrXok-md"
   },
   "outputs": [],
   "source": [
    "def generate_test_data():\n",
    "    # Constants\n",
    "    data_dir = os.path.join(ROOT_DIR, 'data')\n",
    "    indoor3d_data_dir = os.path.join(data_dir, 'stanford_indoor3d')\n",
    "    NUM_POINT = 4096\n",
    "    H5_BATCH_SIZE = 1000\n",
    "    data_dim = [NUM_POINT, 9]\n",
    "    label_dim = [NUM_POINT]\n",
    "    data_dtype = 'float32'\n",
    "    label_dtype = 'uint8'\n",
    "\n",
    "    # Set paths\n",
    "    filelist = os.path.join(ROOT_DIR, 'data/all_data_label.txt')\n",
    "    data_label_files = [os.path.join(indoor3d_data_dir, line.rstrip()) for line in open(filelist)]\n",
    "    output_dir = os.path.join(data_dir, 'indoor3d_sem_seg_hdf5_data_test')\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.mkdir(output_dir)\n",
    "    output_filename_prefix = os.path.join(output_dir, 'ply_data_all')\n",
    "    output_room_filelist = os.path.join(output_dir, 'room_filelist.txt')\n",
    "    output_all_file = os.path.join(output_dir, 'all_files.txt')\n",
    "    fout_room = open(output_room_filelist, 'w')\n",
    "    all_file = open(output_all_file, 'w')\n",
    "\n",
    "    # --------------------------------------\n",
    "    # ----- BATCH WRITE TO HDF5 -----\n",
    "    # --------------------------------------\n",
    "    batch_data_dim = [H5_BATCH_SIZE] + data_dim\n",
    "    batch_label_dim = [H5_BATCH_SIZE] + label_dim\n",
    "    h5_batch_data = np.zeros(batch_data_dim, dtype = np.float32)\n",
    "    h5_batch_label = np.zeros(batch_label_dim, dtype = np.uint8)\n",
    "    buffer_size = 0  # state: record how many samples are currently in buffer\n",
    "    h5_index = 0 # state: the next h5 file to save\n",
    "\n",
    "    def insert_batch(data, label, last_batch=False):\n",
    "        global h5_batch_data, h5_batch_label\n",
    "        global buffer_size, h5_index\n",
    "        data_size = data.shape[0]\n",
    "        # If there is enough space, just insert\n",
    "        if buffer_size + data_size <= h5_batch_data.shape[0]:\n",
    "            h5_batch_data[buffer_size:buffer_size+data_size, ...] = data\n",
    "            h5_batch_label[buffer_size:buffer_size+data_size] = label\n",
    "            buffer_size += data_size\n",
    "        else: # not enough space\n",
    "            capacity = h5_batch_data.shape[0] - buffer_size\n",
    "            assert(capacity>=0)\n",
    "            if capacity > 0:\n",
    "                h5_batch_data[buffer_size:buffer_size+capacity, ...] = data[0:capacity, ...] \n",
    "                h5_batch_label[buffer_size:buffer_size+capacity, ...] = label[0:capacity, ...] \n",
    "            # Save batch data and label to h5 file, reset buffer_size\n",
    "            h5_filename =  output_filename_prefix + '_' + str(h5_index) + '.h5'\n",
    "            save_h5(h5_filename, h5_batch_data, h5_batch_label, data_dtype, label_dtype) \n",
    "            print('Stored {0} with size {1}'.format(h5_filename, h5_batch_data.shape[0]))\n",
    "            h5_index += 1\n",
    "            buffer_size = 0\n",
    "            # recursive call\n",
    "            insert_batch(data[capacity:, ...], label[capacity:, ...], last_batch)\n",
    "        if last_batch and buffer_size > 0:\n",
    "            h5_filename =  output_filename_prefix + '_' + str(h5_index) + '.h5'\n",
    "            save_h5(h5_filename, h5_batch_data[0:buffer_size, ...], h5_batch_label[0:buffer_size, ...], data_dtype, label_dtype)\n",
    "            print('Stored {0} with size {1}'.format(h5_filename, buffer_size))\n",
    "            h5_index += 1\n",
    "            buffer_size = 0\n",
    "        return\n",
    "\n",
    "\n",
    "    sample_cnt = 0\n",
    "    for i, data_label_filename in enumerate(data_label_files):\n",
    "        print(data_label_filename)\n",
    "        data, label = room2blocks_wrapper_normalized(data_label_filename, NUM_POINT, block_size=1.0, stride=1,\n",
    "                                                    random_sample=False, sample_num=None)\n",
    "        print('{0}, {1}'.format(data.shape, label.shape))\n",
    "        for _ in range(data.shape[0]):\n",
    "            fout_room.write(os.path.basename(data_label_filename)[0:-4]+'\\n')\n",
    "\n",
    "        sample_cnt += data.shape[0]\n",
    "        insert_batch(data, label, i == len(data_label_files)-1)\n",
    "\n",
    "    fout_room.close()\n",
    "    # print(\"Total samples: {0}\".format(sample_cnt))\n",
    "\n",
    "    for i in range(h5_index):\n",
    "        all_file.write(os.path.join('indoor3d_sem_seg_hdf5_data_test', 'ply_data_all_') + str(i) +'.h5\\n')\n",
    "    all_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WD0U35U_k-MV"
   },
   "outputs": [],
   "source": [
    "def load_data_semseg(partition, test_area):\n",
    "    DATA_DIR = os.path.join(ROOT_DIR, 'data')\n",
    "    generate_test_data()\n",
    "    if partition == 'train':\n",
    "        data_dir = os.path.join(DATA_DIR, 'indoor3d_sem_seg_hdf5_data')\n",
    "    else:\n",
    "        data_dir = os.path.join(DATA_DIR, 'indoor3d_sem_seg_hdf5_data_test')\n",
    "    with open(os.path.join(data_dir, \"all_files.txt\")) as f:\n",
    "        all_files = [line.rstrip() for line in f]\n",
    "    with open(os.path.join(data_dir, \"room_filelist.txt\")) as f:\n",
    "        room_filelist = [line.rstrip() for line in f]\n",
    "    data_batchlist, label_batchlist = [], []\n",
    "    for f in all_files:\n",
    "        file = h5py.File(os.path.join(DATA_DIR, f), 'r+')\n",
    "        data = file[\"data\"][:]\n",
    "        label = file[\"label\"][:]\n",
    "        data_batchlist.append(data)\n",
    "        label_batchlist.append(label)\n",
    "    data_batches = np.concatenate(data_batchlist, 0)\n",
    "    seg_batches = np.concatenate(label_batchlist, 0)\n",
    "    test_area_name = \"Area_\" + test_area\n",
    "    train_idxs, test_idxs = [], []\n",
    "    for i, room_name in enumerate(room_filelist):\n",
    "        if test_area_name in room_name:\n",
    "            test_idxs.append(i)\n",
    "        else:\n",
    "            train_idxs.append(i)\n",
    "    if partition == 'train':\n",
    "        all_data = data_batches[train_idxs, ...]\n",
    "        all_seg = seg_batches[train_idxs, ...]\n",
    "    else:\n",
    "        all_data = data_batches[test_idxs, ...]\n",
    "        all_seg = seg_batches[test_idxs, ...]\n",
    "    return all_data, all_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A-01eKIIk-Jq"
   },
   "outputs": [],
   "source": [
    "class S3DIS(Dataset):\n",
    "    def __init__(self, num_points=4096, partition='train', test_area='5'):\n",
    "        self.data, self.seg = load_data_semseg(partition, test_area)\n",
    "        self.num_points = num_points\n",
    "        self.partition = partition\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        pointcloud = self.data[item][:self.num_points]\n",
    "        seg = self.seg[item][:self.num_points]\n",
    "        if self.partition == 'train':\n",
    "            indices = list(range(pointcloud.shape[0]))\n",
    "            np.random.shuffle(indices)\n",
    "            pointcloud = pointcloud[indices]\n",
    "            seg = seg[indices]\n",
    "        seg = torch.LongTensor(seg)\n",
    "        pc = torch.as_tensor(pointcloud[...,:6])\n",
    "        pc = pc.permute(1,0)\n",
    "        return pc, seg\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "data_processing_semseg.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
